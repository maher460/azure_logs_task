{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analysis with DuckDB\n",
    "This notebook demonstrates how to use DuckDB to analyze audit and signin logs stored in Parquet files. We will:\n",
    "1. Load the Parquet files into DuckDB tables.\n",
    "2. Inspect the schema and contents of these tables.\n",
    "3. Merge these tables to create an `events` table.\n",
    "4. Create a `sessions` table by aggregating the `events` table.\n",
    "5. Showcase the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import duckdb\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Parquet Files into DuckDB Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where Parquet files are stored\n",
    "auditlogs_dir = \"./parquet_data/insights-logs-auditlogs\"\n",
    "signinlogs_dir = \"./parquet_data/insights-logs-signinlogs\"\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect(\"azure_logs.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auditlogs_combined_file = os.path.join(auditlogs_dir, \"combined_table.parquet\")\n",
    "signinlogs_combined_file = os.path.join(signinlogs_dir, \"combined_table.parquet\")\n",
    "\n",
    "# Drop existing tables if they exist\n",
    "con.execute(\"DROP TABLE IF EXISTS combined_table_auditlogs\")\n",
    "con.execute(\"DROP TABLE IF EXISTS combined_table_signinlogs\")\n",
    "con.execute(\"DROP TABLE IF EXISTS events\")\n",
    "con.execute(\"DROP TABLE IF EXISTS sessions\")\n",
    "\n",
    "# Create tables from Parquet files\n",
    "con.execute(f\"CREATE TABLE combined_table_auditlogs AS SELECT * FROM read_parquet('{auditlogs_combined_file}')\")\n",
    "con.execute(f\"CREATE TABLE combined_table_signinlogs AS SELECT * FROM read_parquet('{signinlogs_combined_file}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect the Tables\n",
    "Let's inspect the schema and first few rows of each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the schema of the audit logs table\n",
    "auditlogs_schema_df = con.execute(\"DESCRIBE SELECT * FROM combined_table_auditlogs\").df()\n",
    "auditlogs_schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the audit logs table\n",
    "auditlogs_sample_df = con.execute(\"SELECT * FROM combined_table_auditlogs LIMIT 10\").df()\n",
    "auditlogs_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the schema of the signin logs table\n",
    "signinlogs_schema_df = con.execute(\"DESCRIBE SELECT * FROM combined_table_signinlogs\").df()\n",
    "signinlogs_schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the signin logs table\n",
    "signinlogs_sample_df = con.execute(\"SELECT * FROM combined_table_signinlogs LIMIT 10\").df()\n",
    "signinlogs_sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Merge Tables to Create `events` Table\n",
    "In this step, we will merge the `combined_table_auditlogs` and `combined_table_signinlogs` tables into a single `events` table. We will rename `correlationId` to `exfSessionId` and select the relevant columns from both tables. We will use the `UNION ALL` operator to combine rows from both `combined_table_auditlogs` and `combined_table_signinlogs` into the `events` table. The `DISTINCT` keyword ensures that duplicate rows are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop existing events table if it exists\n",
    "con.execute(\"DROP TABLE IF EXISTS events\")\n",
    "\n",
    "# Query to merge the tables\n",
    "query = '''\n",
    "CREATE TABLE events AS\n",
    "SELECT DISTINCT\n",
    "    \"time\", \n",
    "    \"operationName\", \n",
    "    \"category\",  \n",
    "    \"correlationId\" AS \"exfSessionId\", \n",
    "    \"callerIpAddress\" AS ipAddress, \n",
    "    \"properties.initiatedBy.user.id\" AS \"userId\", \n",
    "    \"properties.initiatedBy.user.userPrincipalName\" AS \"userPrincipalName\", \n",
    "    \"properties.targetResources.displayName\" AS \"appDisplayName\"\n",
    "FROM combined_table_auditlogs\n",
    "UNION ALL\n",
    "SELECT \n",
    "    \"time\", \n",
    "    \"operationName\", \n",
    "    \"category\", \n",
    "    \"correlationId\" AS \"exfSessionId\", \n",
    "    \"callerIpAddress\" AS ipAddress, \n",
    "    \"properties.userId\" AS \"userId\", \n",
    "    \"properties.userPrincipalName\" AS \"userPrincipalName\", \n",
    "    \"properties.appDisplayName\" AS \"appDisplayName\"\n",
    "FROM combined_table_signinlogs;\n",
    "'''\n",
    "\n",
    "# Execute the query to create the events table\n",
    "con.execute(query)\n",
    "\n",
    "# Verify the results\n",
    "events_df = con.execute(\"SELECT * FROM events\").df()\n",
    "\n",
    "# Save to CSV\n",
    "events_df.to_csv(\"events.csv\", index=False)\n",
    "\n",
    "events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create `sessions` Table by Aggregating the `events` Table\n",
    "In this step, we will create a `sessions` table by aggregating the `events` table based on `exfSessionId`. For each session `exfSessionId`, we will calculate the earliest (`session_start`) and latest (`session_end`) timestamps. We will also determine the most common `userId` and `userPrincipalName` for each session using subqueries with the `GROUP BY` and `ORDER BY` clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop existing sessions table if it exists\n",
    "con.execute(\"DROP TABLE IF EXISTS sessions\")\n",
    "\n",
    "# Query to create the sessions table\n",
    "query3 = '''\n",
    "CREATE TABLE sessions AS\n",
    "SELECT\n",
    "    exfSessionId,\n",
    "    MIN(CAST(\"time\" AS TIMESTAMP)) AS session_start,\n",
    "    MAX(CAST(\"time\" AS TIMESTAMP)) AS session_end,\n",
    "    -- Get the most common values for userId and userPrincipalName\n",
    "    (SELECT userId FROM events e2 WHERE e1.exfSessionId = e2.exfSessionId GROUP BY userId ORDER BY COUNT(*) DESC LIMIT 1) AS userId,\n",
    "    (SELECT userPrincipalName FROM events e2 WHERE e1.exfSessionId = e2.exfSessionId GROUP BY userPrincipalName ORDER BY COUNT(*) DESC LIMIT 1) AS userPrincipalName\n",
    "FROM\n",
    "    events e1\n",
    "GROUP BY\n",
    "    exfSessionId;\n",
    "'''\n",
    "\n",
    "con.execute(query3)\n",
    "\n",
    "# Verify the results\n",
    "sessions_df = con.execute(\"SELECT * FROM sessions\").df()\n",
    "# Save to CSV\n",
    "sessions_df.to_csv(\"sessions.csv\", index=False)\n",
    "sessions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify the Created Tables\n",
    "Let's verify the contents of the `events` and `sessions` tables by checking their schemas and a few sample rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the schema of the events table\n",
    "events_schema_df = con.execute(\"DESCRIBE SELECT * FROM events\").df()\n",
    "events_schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the events table\n",
    "events_sample_df = con.execute(\"SELECT * FROM events LIMIT 10\").df()\n",
    "events_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the schema of the sessions table\n",
    "sessions_schema_df = con.execute(\"DESCRIBE SELECT * FROM sessions\").df()\n",
    "sessions_schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the sessions table\n",
    "sessions_sample_df = con.execute(\"SELECT * FROM sessions LIMIT 10\").df()\n",
    "sessions_sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Close the DuckDB Connection\n",
    "Finally, we close the DuckDB connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DuckDB connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
